The simulation and analysis aims to measure and understand "coercion ratings" given to different types of interaction during interviews. To do this, the experiment uses Python code to generate simulated data across various scenarios like "shoplifting" or "assault".
Where this sits in a broader context is in examining how "interviewer types", which in this case can be "human," "ai," or a "hybrid", affect people's perceptions of coercion when different "strategies" like "pressure" or "rapport" are used. The bigger question here concerns how the source of an interview question, i.e.whether a person or a machine, changes how coercive it is perceived. In a real setting, instead of generating participant effects and ratings using random number generators (rng.normal), the intent is to have actual human participants provide ratings. The approach could be used to test and refine interviewing tools (Human, AI, or Hybrid) by gathering feedback on their perceived coerciveness.

Repository summary asap...
